{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tarea 1 - Amazon food review\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "En primer lugar se carga el corpus, parseando todo código HTML existente con BeautifulSoup. Así solo se mantendrá el contenido que exista en dentro de los tag. Las reseñas se almacenan en la lista **corpus**, la cual tiene una estructura de lista de listas. Corpus[[documento1],[documento2],...]. A su vez, se tiene una lista **score** con la evaluación de los usuarios en cada reseña (esta información será utilizada más adelante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "reviews = open(\"amazon-fine-foods/Reviews.csv\", \"r\")\n",
    "corpus = []\n",
    "score = []\n",
    "\n",
    "reviews.readline() # sacar header\n",
    "\n",
    "for line in reviews:\n",
    "    review = line.strip().split(\",\")\n",
    "    text = BeautifulSoup(review[-1], 'html.parser')\n",
    "    corpus.append([text.get_text()])\n",
    "    score.append(review[6])\n",
    "\n",
    "reviews.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Una vez cargada la información del corpus se procede a tokenizar, manteniendo las palabras sin puntuación alguna. Además se agrega la expresión regular para capturar valores númericos como **3.88**. Esta información se guarda en la lista **tokens_doc** (lista de listas de tokens). Los signos de puntuación también quedan dentro de la lista y serán eliminados en el siguiente paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'I', u'have', u'bought', u'several', u'of', u'the', u'Vitality', u'canned', u'dog', u'food', u'products', u'and', u'have', u'found', u'them', u'all', u'to', u'be', u'of', u'good', u'quality', u'.', u'The', u'product', u'looks', u'more', u'like', u'a', u'stew', u'than', u'a', u'processed', u'meat', u'and', u'it', u'smells', u'better', u'.', u'My', u'Labrador', u'is', u'finicky', u'and', u'she', u'appreciates', u'this', u'product', u'better', u'than', u'most', u'.'], [u'\"', u'Product', u'arrived', u'labeled', u'as', u'Jumbo', u'Salted', u'Peanuts', u'.', u'.', u'.', u'the', u'peanuts', u'were', u'actually', u'small', u'sized', u'unsalted', u'.', u'Not', u'sure', u'if', u'this', u'was', u'an', u'error', u'or', u'if', u'the', u'vendor', u'intended', u'to', u'represent', u'the', u'product', u'as', u'\"', u'\"', u'Jumbo', u'\"', u'\"', u'.', u'\"']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, regexp_tokenize\n",
    "\n",
    "tokens_doc = [] #tokens por documento/sentencia\n",
    "#tokenizer = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "for text in corpus:\n",
    "    #t = tokenizer.tokenize(text[0])\n",
    "    t = regexp_tokenize(text[0], pattern='\\w+|\\$[\\d.]+|\\S')\n",
    "    tokens_doc.append(t)\n",
    "print tokens_doc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos los tokens para cada documento, se carga la lista de stopwords y se procede a filtrar las palabras de cada documento. Los criterios para que no sean eliminados son:\n",
    "\n",
    "* No estar en la stoplist\n",
    "* Largo mayor o igual a 3\n",
    "* Frecuencia de la palabra en el corpus completo sea mayor o igual a 4\n",
    "\n",
    "En este proceso también se aplica *lower case a las palabras*.\n",
    "\n",
    "Los tokens son guardados en la lista tokens, lista de lista de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'bought', u'several', u'vitality', u'canned', u'dog', u'food', u'products', u'found', u'good', u'quality', u'product', u'looks', u'like', u'stew', u'processed', u'meat', u'smells', u'better', u'labrador', u'finicky', u'appreciates', u'product', u'better'], [u'product', u'arrived', u'labeled', u'jumbo', u'salted', u'peanuts', u'peanuts', u'actually', u'small', u'sized', u'unsalted', u'sure', u'error', u'vendor', u'intended', u'represent', u'product', u'jumbo']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_tokens = [token.lower() for doc in tokens_doc for token in doc]\n",
    "\n",
    "stoplist = stopwords.words('english') # stopwords\n",
    "frec_dist = nltk.FreqDist(all_tokens) # frec distribution of tokens\n",
    "\n",
    "# create list of list without stopwords\n",
    "tokens = []\n",
    "for doc in tokens_doc: \n",
    "    t = []\n",
    "    \n",
    "    for token in doc:\n",
    "        low = token.lower()\n",
    "        if low not in stoplist and len(low) >= 3 and frec_dist[low] >= 4:\n",
    "            t.append(low)\n",
    "    tokens.append(t)\n",
    "            \n",
    "print tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'$500.00', u'deductable'), (u'01014', u'c196132y112a'), (u'04830', u'powerpop')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "top_collocations = finder.nbest(bigram_measures.pmi, 30) #top 30 bigramas\n",
    "\n",
    "print top_collocations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'425mgsugar', u'20gcalcium'), (u'45mgpotassium', u'425mgsugar'), (u'4gfiber', u'0gsugar')]\n"
     ]
    }
   ],
   "source": [
    "print top_collocations[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'45mgpotassium' in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'bought', u'several', u'vitality', u'canned', u'dog', u'food', u'products', u'found', u'good', u'quality', u'product', u'looks', u'like', u'stew', u'processed', u'meat', u'smells', u'better', u'labrador', u'finicky', u'appreciates', u'product', u'better']\n",
      "[(u'bought', u'VBD'), (u'several', u'JJ'), (u'vitality', u'NN'), (u'canned', u'VBD'), (u'dog', u'NN'), (u'food', u'NN'), (u'products', u'NNS'), (u'found', u'VBD'), (u'good', u'JJ'), (u'quality', u'NN'), (u'product', u'NN'), (u'looks', u'VBZ'), (u'like', u'IN'), (u'stew', u'NN'), (u'processed', u'JJ'), (u'meat', u'NN'), (u'smells', u'VBZ'), (u'better', u'JJR'), (u'labrador', u'NN'), (u'finicky', u'NN'), (u'appreciates', u'VBZ'), (u'product', u'NN'), (u'better', u'RBR')]\n",
      "pos\n",
      "[[(u'bought', u'VBD'), (u'several', u'JJ'), (u'vitality', u'NN'), (u'canned', u'VBD'), (u'dog', u'NN'), (u'food', u'NN'), (u'products', u'NNS'), (u'found', u'VBD'), (u'good', u'JJ'), (u'quality', u'NN'), (u'product', u'NN'), (u'looks', u'VBZ'), (u'like', u'IN'), (u'stew', u'NN'), (u'processed', u'JJ'), (u'meat', u'NN'), (u'smells', u'VBZ'), (u'better', u'JJR'), (u'labrador', u'NN'), (u'finicky', u'NN'), (u'appreciates', u'VBZ'), (u'product', u'NN'), (u'better', u'RBR')], [(u'product', u'NN'), (u'arrived', u'VBD'), (u'labeled', u'VBN'), (u'jumbo', u'JJ'), (u'salted', u'JJ'), (u'peanuts', u'NNS'), (u'peanuts', u'NNS'), (u'actually', u'RB'), (u'small', u'JJ'), (u'sized', u'VBN'), (u'unsalted', u'JJ'), (u'sure', u'JJ'), (u'error', u'NN'), (u'vendor', u'NN'), (u'intended', u'VBN'), (u'represent', u'VBP'), (u'product', u'NN'), (u'jumbo', u'JJ')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "jar = 'pos-tagger/stanford-postagger.jar'\n",
    "model = 'pos-tagger/english-bidirectional-distsim.tagger'\n",
    "\n",
    "\n",
    "st = StanfordPOSTagger(model,jar)\n",
    "print tokens[0]\n",
    "print st.tag(tokens[0])\n",
    "\n",
    "pos = [st.tag(doc) for doc in tokens[:5000]]\n",
    "\n",
    "print \"pos\"\n",
    "print pos[:2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "jar = 'ner-tagger/stanford-ner.jar'\n",
    "model = 'ner-tagger/english.all.3class.distsim.crf.ser.gz'\n",
    "\n",
    "\n",
    "st = StanfordNERTagger(model, jar) \n",
    "ner = [st.tag(doc) for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sid.polarity_scores(tokens[0]) #probar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
