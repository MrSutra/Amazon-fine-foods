{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tarea 1 - Amazon food review\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "En primer lugar se carga el corpus, quitando todo el código HTML existente con BeautifulSoup, así solo se mantendrá el contenido que exista en dentro de los tag. Las reseñas se almacenan en la lista **corpus**, la cual tiene una estructura de lista de listas *corpus[[documento1],[documento2],...]*. A su vez, se tiene una lista **score** con la evaluación de los usuarios en cada reseña (esta información será utilizada más adelante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "reviews = open(\"amazon-fine-foods/Reviews.csv\", \"r\")\n",
    "corpus = []\n",
    "score = []\n",
    "\n",
    "reviews.readline() # sacar header\n",
    "\n",
    "for line in reviews:\n",
    "    review = line.strip().split(\",\")\n",
    "    text = BeautifulSoup(review[-1], 'html.parser')\n",
    "    corpus.append([text.get_text()])\n",
    "    score.append(review[6])\n",
    "\n",
    "reviews.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Una vez cargada la información del corpus se procede a tokenizar cada documento. La tokenización corresponde a romper una secuencia de strings en piezas, en ese caso será en palabras. Mediante expresión regular se quita todo número y signo de puntuación existente, quedando así, solo letras.\n",
    "Esta información se guarda en la lista **tokens_doc** (lista de listas de tokens). \n",
    "\n",
    "\n",
    "En este proceso también se aplica *lower case a las palabras*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'i', u'have', u'bought', u'several', u'of', u'the', u'vitality', u'canned', u'dog', u'food', u'products', u'and', u'have', u'found', u'them', u'all', u'to', u'be', u'of', u'good', u'quality', u'the', u'product', u'looks', u'more', u'like', u'a', u'stew', u'than', u'a', u'processed', u'meat', u'and', u'it', u'smells', u'better', u'my', u'labrador', u'is', u'finicky', u'and', u'she', u'appreciates', u'this', u'product', u'better', u'than', u'most'], [u'product', u'arrived', u'labeled', u'as', u'jumbo', u'salted', u'peanuts', u'the', u'peanuts', u'were', u'actually', u'small', u'sized', u'unsalted', u'not', u'sure', u'if', u'this', u'was', u'an', u'error', u'or', u'if', u'the', u'vendor', u'intended', u'to', u'represent', u'the', u'product', u'as', u'jumbo']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, regexp_tokenize\n",
    "\n",
    "tokens_doc = [] #tokens por documento/sentencia\n",
    "\n",
    "for text in corpus:\n",
    "    t = regexp_tokenize(text[0].lower(), pattern='[a-zA-Z]+')\n",
    "    tokens_doc.append(t)\n",
    "print tokens_doc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Una vez que tenemos los tokens para cada documento, se carga la lista de stopwords desde la librería de nltk. Las stopwords son aquellas palabras que no son descriptivas para el corpus.\n",
    "Una vez que se realizó el filtrado de las stopwords de nltk se vió que aún existian palabras que no permitían un buen analisis, que no entregaban información relevante, por lo que se creo un archivo con aquellas stopwords que nosotros observamos, *'stopwrds.txt*. Este archivo lo juntamos con las anteriores y se procede a filtrar las palabras de cada documento. Los criterio para que no sean eliminados son:\n",
    "\n",
    "* No estar en la stoplist\n",
    "* Largo mayor o igual a 3\n",
    "* Frecuencia de la palabra en el corpus completo sea mayor o igual a 4\n",
    "\n",
    "Los tokens son guardados en la lista **tokens**, lista de lista de tokens. En base a esta lista, se crea otra lista, llamada **all_tokens** la cual contiene todos los tokens del corpus, sin separación entre documentosla cual nos permite generar la distribución de las palabras en el corpus según su frecuencia, y así poder utilizar esto como criterio de filtrado.\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'bought', u'several', u'vitality', u'canned', u'dog', u'food', u'products', u'found', u'good', u'quality', u'product', u'looks', u'like', u'stew', u'processed', u'meat', u'smells', u'better', u'labrador', u'finicky', u'appreciates', u'product', u'better'], [u'product', u'arrived', u'labeled', u'jumbo', u'salted', u'peanuts', u'peanuts', u'actually', u'small', u'sized', u'unsalted', u'sure', u'error', u'vendor', u'intended', u'represent', u'product', u'jumbo']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# making stoplist with custom stopwords and library stopwords\n",
    "stoplist = []\n",
    "f = open(\"stopwords.txt\",\"r\")\n",
    "for sw in f:\n",
    "    stoplist.append(sw.strip())\n",
    "f.close()\n",
    "stoplist += stopwords.words('english') # stopwords\n",
    "\n",
    "\n",
    "all_tokens = [token for doc in tokens_doc for token in doc]\n",
    "\n",
    "frec_dist = nltk.FreqDist(all_tokens) # frec distribution of tokens\n",
    "\n",
    "# create list of list without stopwords\n",
    "tokens = []\n",
    "for doc in tokens_doc: \n",
    "    t = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token not in stoplist and len(token) >= 3 and frec_dist[token] >= 4:\n",
    "            t.append(token)\n",
    "    tokens.append(t)\n",
    "            \n",
    "print tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# COMPLETAR (rellenar con bla bla la comparación)\n",
    "\n",
    "Se vuelve a generar la lista **all_tokens**, ya que en el paso anterior se eliminaron algunas palabras, por lo que se debe actualizar.\n",
    "El objetivo de este paso es generar el top-30 collocations, formadas por bigramas. Las collocations son aquellas  palabras que generalmente van juntas dentro de un texto, la cantidad de estas palabras se define según el n-grama usado, este caso es de a dos palabras, por lo tanto se deben buscar los 30 pares de palabras que generalmente van juntas dentro de las reviews de comida de amazon.\n",
    "Para lograr este objetivo se ejecutan 4 métricas de proximidad entre las palabras:\n",
    "\n",
    "* likelihood_ratio: Utiliza los ratios de verosimilitud en los bigramas.\n",
    "* raw_freq: Usa la frecuencia, y considera todos los bigramas en el texto como candidatos de collocations.\n",
    "* pmi: Usa Pointwise Mutual Information.\n",
    "* chi_sq: Determina los bigramas utilizando la distribución chi-cuadrado.\n",
    "\n",
    "Comparando estos resultados, se observa que *likelihood_ratio* entrega un mejor resultado. Luego, le sigue *raw_freq* y *pmi*. Finalmente, la que entrega peores resultados es la *chi_sq*.\n",
    "\n",
    "En general los resultados de *likelihood_ratio* y *raw_freq* son casi iguales, cambiando el orden de algunos bigramas en la lista de top 30. Se pueden distinguir claramente combinaciones de palabras que uno esperaría encontrar en un dataset que habla sobre comida, como por ejemplo, gluten free, peanut butter, grocery store. Otros más obvios como Amazon com, especialmente porque la información proviene de Amazon.com. \n",
    "\n",
    "Por otro lado, *pmi* y *chi_sq* entregan resultados de muy mala calidad. En general son palabras sin sentido y que como bigramas no significan nada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'highly', u'recommend'), (u'gluten', u'free'), (u'peanut', u'butter'), (u'subscribe', u'save'), (u'grocery', u'store'), (u'highly', u'recommended'), (u'much', u'better'), (u'would', u'recommend'), (u'year', u'old'), (u'amazon', u'com'), (u'green', u'mountain'), (u'green', u'tea'), (u'cup', u'coffee'), (u'hard', u'find'), (u'waste', u'money'), (u'free', u'shipping'), (u'dog', u'food'), (u'expiration', u'date'), (u'earl', u'grey'), (u'recommend', u'anyone'), (u'long', u'time'), (u'tastes', u'like'), (u'olive', u'oil'), (u'customer', u'service'), (u'dark', u'chocolate'), (u'ice', u'cream'), (u'years', u'ago'), (u'every', u'day'), (u'make', u'sure'), (u'give', u'try')] \n",
      "\n",
      "[((u'highly', u'recommend'), 0.0015492917887110697), ((u'would', u'recommend'), 0.0008041488021960918), ((u'gluten', u'free'), 0.0008017432893265979), ((u'great', u'product'), 0.0007933947446618839), ((u'much', u'better'), 0.0007659435977982482), ((u'taste', u'like'), 0.0007244838759887366), ((u'grocery', u'store'), 0.0006422719361548584), ((u'peanut', u'butter'), 0.0006200563173012976), ((u'cup', u'coffee'), 0.0005920391674095457), ((u'highly', u'recommended'), 0.0005892091522689646), ((u'really', u'good'), 0.0005710970553692462), ((u'great', u'price'), 0.0005532679599835859), ((u'tastes', u'like'), 0.0005469004259172787), ((u'subscribe', u'save'), 0.000533599354756548), ((u'taste', u'good'), 0.0005153457570998004), ((u'dog', u'food'), 0.0005094027253045803), ((u'recommend', u'product'), 0.000502469188210157), ((u'taste', u'great'), 0.0004709145193926787), ((u'green', u'tea'), 0.0004615754694287614), ((u'would', u'buy'), 0.00045648144217571565), ((u'tastes', u'great'), 0.0004470008914547693), ((u'really', u'like'), 0.0004249267733582375), ((u'hard', u'find'), 0.000415021720366204), ((u'amazon', u'com'), 0.00039040058864314925), ((u'long', u'time'), 0.0003770995174824185), ((u'free', u'shipping'), 0.00037228849174343083), ((u'year', u'old'), 0.00037073198341611125), ((u'good', u'price'), 0.0003705904826590822), ((u'give', u'try'), 0.00036181743572328114), ((u'dogs', u'love'), 0.0003538933933296543)] \n",
      "\n",
      "[((u'antioxdent', u'ithelps'), 20.75268689271526), ((u'aura', u'electromagnetic'), 20.75268689271526), ((u'baltasar', u'gracian'), 20.75268689271526), ((u'bicardi', u'oakheart'), 20.75268689271526), ((u'brownieok', u'flavorspretty'), 20.75268689271526), ((u'camellia', u'sinensis'), 20.75268689271526), ((u'cheeaper', u'petsmarthowever'), 20.75268689271526), ((u'craggles', u'wondermentsperfection'), 20.75268689271526), ((u'crags', u'craggles'), 20.75268689271526), ((u'dandie', u'dinmont'), 20.75268689271526), ((u'debbiednorvell', u'insightbb'), 20.75268689271526), ((u'earthquakes', u'nocks'), 20.75268689271526), ((u'eity', u'bity'), 20.75268689271526), ((u'gynostemma', u'pentaphyllum'), 20.75268689271526), ((u'hempnutrition', u'pdfhttp'), 20.75268689271526), ((u'hurly', u'burly'), 20.75268689271526), ((u'jacquie', u'woodsmaui'), 20.75268689271526), ((u'mantienen', u'aliento'), 20.75268689271526), ((u'moneyrip', u'offpopcorn'), 20.75268689271526), ((u'nodoz', u'vivarin'), 20.75268689271526), ((u'nosalty', u'soexpensive'), 20.75268689271526), ((u'optic', u'neuritis'), 20.75268689271526), ((u'orbs', u'crags'), 20.75268689271526), ((u'pdfhttp', u'sudburymarket'), 20.75268689271526), ((u'peking', u'duckin'), 20.75268689271526), ((u'perritos', u'mantienen'), 20.75268689271526), ((u'rooo', u'vah'), 20.75268689271526), ((u'sak', u'ouwt'), 20.75268689271526), ((u'sheng', u'tangshen'), 20.75268689271526), ((u'sincerelydanny', u'knowles'), 20.75268689271526)] \n",
      "\n",
      "[((u'antioxdent', u'ithelps'), 7067100.0), ((u'aura', u'electromagnetic'), 7067100.0), ((u'ballgreat', u'trainingconveniently'), 7067100.0), ((u'baltasar', u'gracian'), 7067100.0), ((u'baskin', u'robbins'), 7067100.0), ((u'behren', u'reneofc'), 7067100.0), ((u'bicardi', u'oakheart'), 7067100.0), ((u'blum', u'jablum'), 7067100.0), ((u'boletus', u'edulis'), 7067100.0), ((u'brownieok', u'flavorspretty'), 7067100.0), ((u'buttermaple', u'walnutdark'), 7067100.0), ((u'camellia', u'sinensis'), 7067100.0), ((u'cantidad', u'supongo'), 7067100.0), ((u'carboardy', u'styrofoamy'), 7067100.0), ((u'carbslower', u'fatcons'), 7067100.0), ((u'cazzo', u'ogni'), 7067100.0), ((u'cheeaper', u'petsmarthowever'), 7067100.0), ((u'chrissy', u'mcvay'), 7067100.0), ((u'coffeeexcellent', u'servicebest'), 7067100.0), ((u'contraction', u'epidural'), 7067100.0), ((u'craggles', u'wondermentsperfection'), 7067100.0), ((u'crags', u'craggles'), 7067100.0), ((u'dandie', u'dinmont'), 7067100.0), ((u'debbiednorvell', u'insightbb'), 7067100.0), ((u'dippingcheddar', u'yummyparmesan'), 7067100.0), ((u'earthquakes', u'nocks'), 7067100.0), ((u'eity', u'bity'), 7067100.0), ((u'enterococcus', u'faecium'), 7067100.0), ((u'fatcons', u'expensiveclumpier'), 7067100.0), ((u'forma', u'pida'), 7067100.0)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "all_tokens = [token for doc in tokens for token in doc]\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "#print finder.items()[0:5]\n",
    "\n",
    "top_collocations = finder.nbest(bigram_measures.likelihood_ratio, 30) #top 30 bigramas\n",
    "print top_collocations[:30], \"\\n\"\n",
    "\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print scored[:30], \"\\n\"\n",
    "\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "print scored[:30], \"\\n\"\n",
    "\n",
    "scored = finder.score_ngrams(bigram_measures.chi_sq)\n",
    "print scored[:30], \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Models POS tagger\n",
    "wsj-0-18-bidirectional-distsim.tagger\n",
    "Trained on WSJ sections 0-18 using a bidirectional architecture and\n",
    "including word shape and distributional similarity features.\n",
    "Penn Treebank tagset.\n",
    "Performance:\n",
    "97.28% correct on WSJ 19-21\n",
    "(90.46% correct on unknown words)\n",
    "\n",
    "wsj-0-18-left3words.tagger\n",
    "Trained on WSJ sections 0-18 using the left3words architecture and\n",
    "includes word shape features.  Penn tagset.\n",
    "Performance:\n",
    "96.97% correct on WSJ 19-21\n",
    "(88.85% correct on unknown words)\n",
    "\n",
    "wsj-0-18-left3words-distsim.tagger\n",
    "Trained on WSJ sections 0-18 using the left3words architecture and\n",
    "includes word shape and distributional similarity features. Penn tagset.\n",
    "Performance:\n",
    "97.01% correct on WSJ 19-21\n",
    "(89.81% correct on unknown words)\n",
    "\n",
    "english-left3words-distsim.tagger\n",
    "Trained on WSJ sections 0-18 and extra parser training data using the\n",
    "left3words architecture and includes word shape and distributional\n",
    "similarity features. Penn tagset.\n",
    "\n",
    "english-bidirectional-distsim.tagger\n",
    "Trained on WSJ sections 0-18 using a bidirectional architecture and\n",
    "including word shape and distributional similarity features.\n",
    "Penn Treebank tagset.\n",
    "\n",
    "wsj-0-18-caseless-left3words-distsim.tagger\n",
    "Trained on WSJ sections 0-18 left3words architecture and includes word\n",
    "shape and distributional similarity features. Penn tagset.  Ignores case.\n",
    "\n",
    "english-caseless-left3words-distsim.tagger\n",
    "Trained on WSJ sections 0-18 and extra parser training data using the\n",
    "left3words architecture and includes word shape and distributional\n",
    "similarity features. Penn tagset.  Ignores case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "POS\n",
    "\n",
    "El proceso POS (Part-Of-Speech) tagger, se encarga de etiquetar cada uno de los tokens de acuerdo a la función que cumple en el texto, su categoria gramatical, por ejemplo si un token es un verbo, sustantivo, etc.\n",
    "\n",
    "Tanto para POS y NER tagger utilizamos el POS tagger de Standford, en el cual se debe especificar el ejecutable jar Standford en Java y el modelo entrenado según el cuál se analizará el corpus actual. \n",
    "\n",
    "Los modelos entregados para POS tagger están basados en el Wall Street Journal, teniendo distintas caracteristicas cada uno de ellos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'bought', u'several', u'vitality', u'canned', u'dog', u'food', u'products', u'found', u'good', u'quality', u'product', u'looks', u'like', u'stew', u'processed', u'meat', u'smells', u'better', u'labrador', u'finicky', u'appreciates', u'product', u'better']\n",
      "[(u'bought', u'VBD'), (u'several', u'JJ'), (u'vitality', u'NN'), (u'canned', u'VBD'), (u'dog', u'NN'), (u'food', u'NN'), (u'products', u'NNS'), (u'found', u'VBD'), (u'good', u'JJ'), (u'quality', u'NN'), (u'product', u'NN'), (u'looks', u'VBZ'), (u'like', u'IN'), (u'stew', u'NN'), (u'processed', u'JJ'), (u'meat', u'NN'), (u'smells', u'VBZ'), (u'better', u'JJR'), (u'labrador', u'NN'), (u'finicky', u'NN'), (u'appreciates', u'VBZ'), (u'product', u'NN'), (u'better', u'RBR')]\n",
      "pos\n",
      "[[(u'bought', u'VBD'), (u'several', u'JJ'), (u'vitality', u'NN'), (u'canned', u'VBD'), (u'dog', u'NN'), (u'food', u'NN'), (u'products', u'NNS'), (u'found', u'VBD'), (u'good', u'JJ'), (u'quality', u'NN'), (u'product', u'NN'), (u'looks', u'VBZ'), (u'like', u'IN'), (u'stew', u'NN'), (u'processed', u'JJ'), (u'meat', u'NN'), (u'smells', u'VBZ'), (u'better', u'JJR'), (u'labrador', u'NN'), (u'finicky', u'NN'), (u'appreciates', u'VBZ'), (u'product', u'NN'), (u'better', u'RBR')], [(u'product', u'NN'), (u'arrived', u'VBD'), (u'labeled', u'VBN'), (u'jumbo', u'JJ'), (u'salted', u'JJ'), (u'peanuts', u'NNS'), (u'peanuts', u'NNS'), (u'actually', u'RB'), (u'small', u'JJ'), (u'sized', u'VBN'), (u'unsalted', u'JJ'), (u'sure', u'JJ'), (u'error', u'NN'), (u'vendor', u'NN'), (u'intended', u'VBN'), (u'represent', u'VBP'), (u'product', u'NN'), (u'jumbo', u'JJ')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "jar = 'pos-tagger/stanford-postagger.jar'\n",
    "model = 'pos-tagger/english-bidirectional-distsim.tagger'\n",
    "\n",
    "\n",
    "st = StanfordPOSTagger(model,jar)\n",
    "print tokens[0]\n",
    "print st.tag(tokens[0])\n",
    "\n",
    "pos = [st.tag(doc) for doc in tokens[:5000]]\n",
    "\n",
    "print \"pos\"\n",
    "print pos[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "NER\n",
    "\n",
    "El proceso NER (Named Entity Recognizer) tagger, se encarga de identificar entidades dentro del texto, es decir si es una organización, persona, etc. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "jar = 'ner-tagger/stanford-ner.jar'\n",
    "model = 'ner-tagger/english.all.3class.distsim.crf.ser.gz'\n",
    "\n",
    "\n",
    "st = StanfordNERTagger(model, jar) \n",
    "ner = [st.tag(doc) for doc in corpus[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(u'I', u'O')], [(u'\"Product', u'O')]]\n",
      "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'\"Product', u'O'),\n",
       " (u'arrived', u'O'),\n",
       " (u'labeled', u'O'),\n",
       " (u'as', u'O'),\n",
       " (u'Jumbo', u'O'),\n",
       " (u'Salted', u'O'),\n",
       " (u'Peanuts...the', u'O'),\n",
       " (u'peanuts', u'O'),\n",
       " (u'were', u'O'),\n",
       " (u'actually', u'O'),\n",
       " (u'small', u'O'),\n",
       " (u'sized', u'O'),\n",
       " (u'unsalted.', u'O'),\n",
       " (u'Not', u'O'),\n",
       " (u'sure', u'O'),\n",
       " (u'if', u'O'),\n",
       " (u'this', u'O'),\n",
       " (u'was', u'O'),\n",
       " (u'an', u'O'),\n",
       " (u'error', u'O'),\n",
       " (u'or', u'O'),\n",
       " (u'if', u'O'),\n",
       " (u'the', u'O'),\n",
       " (u'vendor', u'O'),\n",
       " (u'intended', u'O'),\n",
       " (u'to', u'O'),\n",
       " (u'represent', u'O'),\n",
       " (u'the', u'O'),\n",
       " (u'product', u'O'),\n",
       " (u'as', u'O'),\n",
       " (u'\"\"Jumbo\"\".\"', u'O')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ner\n",
    "print corpus[0][0]\n",
    "st.tag(corpus[1][0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "sentiment analysis\n",
    "\n",
    "Finalmente se realiza un análisis de sentimiento de los reviews, es decir se determina cuan negativa, neutral y positiva es, entregando un resultado final que pondera estas tres meétricas y entrega un puntaje de sentiment analysis.\n",
    "\n",
    "Para este propósito se utiliza Vader, dado que es un analizador se sentimientos basado en reglas que están específicamente en sintonía con los sentimientos expresados en los medios sociales. Así, para un corpus como el nuestro, que está escrito en un entorno en que se entrega opiniones como generalmente ocurre en redes soiales, funciona bien. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "polarity = []\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in tokens:\n",
    "    for token in sentence:\n",
    "        polarity.append(sid.polarity_scores(token)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.3182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4404, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print polarity[:10]\n",
    "print "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
