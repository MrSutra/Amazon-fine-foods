{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1 - Amazon food review\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "En primer lugar se importarán todas las librerías a utilizar:\n",
    "BeautifulSoup: Para eliminar el código HTML de la reseña.\n",
    "nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "reviews = open(\"amazon-fine-foods/Reviews.csv\", \"r\")\n",
    "corpus = []\n",
    "score = []\n",
    "\n",
    "reviews.readline() # sacar header\n",
    "\n",
    "for line in reviews:\n",
    "    review = line.strip().split(\",\")\n",
    "    text = BeautifulSoup(review[-1], 'html.parser')\n",
    "    corpus.append([text.get_text()])\n",
    "    score.append(review[6])\n",
    "\n",
    "reviews.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'I', u'have', u'bought', u'several', u'of', u'the', u'Vitality', u'canned', u'dog', u'food', u'products', u'and', u'have', u'found', u'them', u'all', u'to', u'be', u'of', u'good', u'quality', u'.', u'The', u'product', u'looks', u'more', u'like', u'a', u'stew', u'than', u'a', u'processed', u'meat', u'and', u'it', u'smells', u'better', u'.', u'My', u'Labrador', u'is', u'finicky', u'and', u'she', u'appreciates', u'this', u'product', u'better', u'than', u'most', u'.'], [u'\"', u'Product', u'arrived', u'labeled', u'as', u'Jumbo', u'Salted', u'Peanuts', u'.', u'.', u'.', u'the', u'peanuts', u'were', u'actually', u'small', u'sized', u'unsalted', u'.', u'Not', u'sure', u'if', u'this', u'was', u'an', u'error', u'or', u'if', u'the', u'vendor', u'intended', u'to', u'represent', u'the', u'product', u'as', u'\"', u'\"', u'Jumbo', u'\"', u'\"', u'.', u'\"']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, regexp_tokenize\n",
    "\n",
    "tokens_doc = [] #tokens por documento/sentenci\n",
    "#tokenizer = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "for text in corpus:\n",
    "    #t = tokenizer.tokenize(text[0])\n",
    "    t = regexp_tokenize(text[0], pattern='\\w+|\\$[\\d.]+|\\S')\n",
    "    tokens_doc.append(t)\n",
    "print tokens_doc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'bought', u'several', u'vitality', u'canned', u'dog', u'food', u'products', u'found', u'good', u'quality', u'product', u'looks', u'like', u'stew', u'processed', u'meat', u'smells', u'better', u'labrador', u'finicky', u'appreciates', u'product', u'better'], [u'product', u'arrived', u'labeled', u'jumbo', u'salted', u'peanuts', u'peanuts', u'actually', u'small', u'sized', u'unsalted', u'sure', u'error', u'vendor', u'intended', u'represent', u'product', u'jumbo']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_tokens = [token.lower() for doc in tokens_doc for token in doc]\n",
    "\n",
    "stoplist = stopwords.words('english') # stopwords\n",
    "frec_dist = nltk.FreqDist(all_tokens) # frec distribution of tokens\n",
    "\n",
    "# create list of list without stopwords\n",
    "tokens = []\n",
    "for doc in tokens_doc: \n",
    "    t = []\n",
    "    \n",
    "    for token in doc:\n",
    "        low = token.lower()\n",
    "        if low not in stoplist and len(low) >= 3 and frec_dist[low] >= 4:\n",
    "            t.append(low)\n",
    "    tokens.append(t)\n",
    "            \n",
    "print tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'$500.00', u'deductable'), (u'01014', u'c196132y112a'), (u'04830', u'powerpop')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "top_collocations = finder.nbest(bigram_measures.pmi, 30) #top 30 bigramas\n",
    "\n",
    "print top_collocations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'425mgsugar', u'20gcalcium'), (u'45mgpotassium', u'425mgsugar'), (u'4gfiber', u'0gsugar')]\n"
     ]
    }
   ],
   "source": [
    "print top_collocations[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'nosalty' in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
