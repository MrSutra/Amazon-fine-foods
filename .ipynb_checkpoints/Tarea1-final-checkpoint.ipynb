{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tarea 1 - Amazon food review\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "En primer lugar se carga el corpus, quitando todo el código HTML existente con BeautifulSoup, así solo se mantendrá el contenido que exista en dentro de los tag. Las reseñas se almacenan en la lista **corpus**, la cual tiene una estructura de lista de listas *corpus[[documento1],[documento2],...]*. A su vez, se tiene una lista **score** con la evaluación de los usuarios en cada reseña (esta información será utilizada más adelante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "reviews = open(\"amazon-fine-foods/Reviews.csv\", \"r\")\n",
    "corpus = []\n",
    "score = []\n",
    "\n",
    "reviews.readline() # sacar header\n",
    "\n",
    "for line in reviews:\n",
    "    review = line.strip().split(\",\")\n",
    "    text = BeautifulSoup(review[-1], 'html.parser')\n",
    "    corpus.append([text.get_text()])\n",
    "    score.append(review[6])\n",
    "\n",
    "reviews.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Una vez cargada la información del corpus se procede a tokenizar cada documento. La tokenización corresponde a romper una secuencia de strings en piezas, en ese caso será en palabras. Mediante expresión regular se quita todo número y signo de puentuación existente, quedando así, solo letras.\n",
    "Esta información se guarda en la lista **tokens_doc** (lista de listas de tokens). \n",
    "\n",
    "\n",
    "En este proceso también se aplica *lower case a las palabras*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'i', u'have', u'bought', u'several', u'of', u'the', u'vitality', u'canned', u'dog', u'food', u'products', u'and', u'have', u'found', u'them', u'all', u'to', u'be', u'of', u'good', u'quality', u'the', u'product', u'looks', u'more', u'like', u'a', u'stew', u'than', u'a', u'processed', u'meat', u'and', u'it', u'smells', u'better', u'my', u'labrador', u'is', u'finicky', u'and', u'she', u'appreciates', u'this', u'product', u'better', u'than', u'most'], [u'product', u'arrived', u'labeled', u'as', u'jumbo', u'salted', u'peanuts', u'the', u'peanuts', u'were', u'actually', u'small', u'sized', u'unsalted', u'not', u'sure', u'if', u'this', u'was', u'an', u'error', u'or', u'if', u'the', u'vendor', u'intended', u'to', u'represent', u'the', u'product', u'as', u'jumbo']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, regexp_tokenize\n",
    "\n",
    "tokens_doc = [] #tokens por documento/sentencia\n",
    "#tokenizer = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "for text in corpus:\n",
    "    t = regexp_tokenize(text[0].lower(), pattern='[a-zA-Z]+')\n",
    "    tokens_doc.append(t)\n",
    "print tokens_doc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Una vez que tenemos los tokens para cada documento, se carga la lista de stopwords desde la librería de nltk. Las stopwords son aquellas palabras que no son descriptivas para el corpus.\n",
    "Una vez que se realizó el filtrado de las stopwords de nltk se vió que aún existian palabras que no permitían un buen analisis, que no entregaban información relevante, por lo que se creo un archivo con aquellas stopwords que nosotros observamos, *'stopwrds.txt*. Este archivo lo juntamos con las anteriorores y se procede a filtrar las palabras de cada documento. Los criterios para que no sean eliminados son:\n",
    "\n",
    "* No estar en la stoplist\n",
    "* Largo mayor o igual a 3\n",
    "* Frecuencia de la palabra en el corpus completo sea mayor o igual a 4\n",
    "\n",
    "Los tokens son guardados en la lista **tokens**, lista de lista de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gag', 'b001kt61p8', 'fri', 'fro', 'frm', 'fre', 'fru', 'hah', 'hae', 'hay', 'har', 'hav', 'haw', 'yummmmm', 'booo', 'taj', '_too_', 'cfl', 'cfh', 'a__', '1tbsp', 'scwt', '$...', '000', '090costco', u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']\n",
      "[[u'bought', u'several', u'vitality', u'canned', u'dog', u'food', u'products', u'found', u'good', u'quality', u'product', u'looks', u'like', u'stew', u'processed', u'meat', u'smells', u'better', u'labrador', u'finicky', u'appreciates', u'product', u'better'], [u'product', u'arrived', u'labeled', u'jumbo', u'salted', u'peanuts', u'peanuts', u'actually', u'small', u'sized', u'unsalted', u'sure', u'error', u'vendor', u'intended', u'represent', u'product', u'jumbo']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# making stoplist with custom stopwords and library stopwords\n",
    "stoplist = []\n",
    "f = open(\"stopwords.txt\",\"r\")\n",
    "for sw in f:\n",
    "    stoplist.append(sw.strip())\n",
    "f.close()\n",
    "stoplist += stopwords.words('english') # stopwords\n",
    "\n",
    "\n",
    "all_tokens = [token for doc in tokens_doc for token in doc]\n",
    "\n",
    "frec_dist = nltk.FreqDist(all_tokens) # frec distribution of tokens\n",
    "\n",
    "# create list of list without stopwords\n",
    "tokens = []\n",
    "for doc in tokens_doc: \n",
    "    t = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token not in stoplist and len(token) >= 3 and frec_dist[token] >= 4:\n",
    "            t.append(token)\n",
    "    tokens.append(t)\n",
    "            \n",
    "print tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Collocations\n",
    "\n",
    "\n",
    "Luego de tener l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'highly', u'recommend'), (u'gluten', u'free'), (u'peanut', u'butter'), (u'subscribe', u'save'), (u'grocery', u'store'), (u'highly', u'recommended'), (u'much', u'better'), (u'would', u'recommend'), (u'year', u'old'), (u'amazon', u'com'), (u'green', u'mountain'), (u'green', u'tea'), (u'cup', u'coffee'), (u'hard', u'find'), (u'waste', u'money'), (u'free', u'shipping'), (u'dog', u'food'), (u'expiration', u'date'), (u'earl', u'grey'), (u'recommend', u'anyone'), (u'long', u'time'), (u'tastes', u'like'), (u'olive', u'oil'), (u'customer', u'service'), (u'dark', u'chocolate'), (u'ice', u'cream'), (u'years', u'ago'), (u'every', u'day'), (u'make', u'sure'), (u'give', u'try')]\n",
      "[((u'antioxdent', u'ithelps'), 7067100.0), ((u'aura', u'electromagnetic'), 7067100.0), ((u'ballgreat', u'trainingconveniently'), 7067100.0), ((u'baltasar', u'gracian'), 7067100.0), ((u'baskin', u'robbins'), 7067100.0), ((u'behren', u'reneofc'), 7067100.0), ((u'bicardi', u'oakheart'), 7067100.0), ((u'blum', u'jablum'), 7067100.0), ((u'boletus', u'edulis'), 7067100.0), ((u'brownieok', u'flavorspretty'), 7067100.0), ((u'buttermaple', u'walnutdark'), 7067100.0), ((u'camellia', u'sinensis'), 7067100.0), ((u'cantidad', u'supongo'), 7067100.0), ((u'carboardy', u'styrofoamy'), 7067100.0), ((u'carbslower', u'fatcons'), 7067100.0), ((u'cazzo', u'ogni'), 7067100.0), ((u'cheeaper', u'petsmarthowever'), 7067100.0), ((u'chrissy', u'mcvay'), 7067100.0), ((u'coffeeexcellent', u'servicebest'), 7067100.0), ((u'contraction', u'epidural'), 7067100.0), ((u'craggles', u'wondermentsperfection'), 7067100.0), ((u'crags', u'craggles'), 7067100.0), ((u'dandie', u'dinmont'), 7067100.0), ((u'debbiednorvell', u'insightbb'), 7067100.0), ((u'dippingcheddar', u'yummyparmesan'), 7067100.0), ((u'earthquakes', u'nocks'), 7067100.0), ((u'eity', u'bity'), 7067100.0), ((u'enterococcus', u'faecium'), 7067100.0), ((u'fatcons', u'expensiveclumpier'), 7067100.0), ((u'forma', u'pida'), 7067100.0)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "all_tokens = [token for doc in tokens for token in doc]\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "\n",
    "top_collocations = finder.nbest(bigram_measures.likelihood_ratio, 30) #top 30 bigramas\n",
    "print top_collocations[:30]\n",
    "\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print scored[:30]\n",
    "\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "print scored[:30]\n",
    "\n",
    "scored = finder.score_ngrams(bigram_measures.chi_sq)\n",
    "print scored[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print top_collocations[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "jar = 'pos-tagger/stanford-postagger.jar'\n",
    "model = 'pos-tagger/english-bidirectional-distsim.tagger'\n",
    "\n",
    "\n",
    "st = StanfordPOSTagger(model,jar)\n",
    "print tokens[0]\n",
    "print st.tag(tokens[0])\n",
    "\n",
    "pos = [st.tag(doc) for doc in tokens[:5000]]\n",
    "\n",
    "print \"pos\"\n",
    "print pos[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "jar = 'ner-tagger/stanford-ner.jar'\n",
    "model = 'ner-tagger/english.all.3class.distsim.crf.ser.gz'\n",
    "\n",
    "\n",
    "st = StanfordNERTagger(model, jar) \n",
    "ner = [st.tag(doc) for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sid.polarity_scores(tokens[0][0]) #probar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
